{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/train.csv')\n",
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "y = np.array([a2c[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Few Preprocessings\n",
    "\n",
    "In traditional NLP tasks, preprocessings play an important role, but...\n",
    "\n",
    "## Low-frequency words\n",
    "\n",
    "In my experience, fastText is very fast, but I need to delete rare words to avoid overfitting.\n",
    "\n",
    "NOTE: Some keywords are rare words, such like Cthulhu in Cthulhu Mythos of Howard Phillips Lovecraft. But these are useful for this task.\n",
    "\n",
    "## Removing Stopwords\n",
    "\n",
    "Nothing. To identify author from a sentence, some stopwords play an important role because one has specific usages of them.\n",
    "\n",
    "## Stemming and Lowercase\n",
    "\n",
    "Nothing. This reason is the same for stopwords removing. And I guess some stemming rules provided by libraries is bad for this task because all author is the older author.\n",
    "\n",
    "## Cutting long sentence\n",
    "\n",
    "Too long documents are cut.\n",
    "\n",
    "## Punctuation\n",
    "\n",
    "Because I guess each author has unique punctuations's usage in the novel, I separate them from words.\n",
    "\n",
    "e.g. Don't worry -> Don ' t worry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character didtribution per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c EAP   MWS   HPL   \n",
      "M 1065 415 645 \n",
      "t 82426 63142 62235 \n",
      "' 1334 476 1710 \n",
      "J 164 66 210 \n",
      "ἶ 0 0 2 \n",
      "æ 36 0 10 \n",
      "Π 0 0 1 \n",
      "i 60952 46080 44250 \n",
      "q 1030 677 779 \n",
      "â 6 0 0 \n",
      "n 62636 50291 50879 \n",
      "H 864 669 741 \n",
      "E 435 445 281 \n",
      "ñ 0 0 7 \n",
      "m 22792 20471 17622 \n",
      "N 411 204 345 \n",
      "l 35371 27819 30273 \n",
      "Σ 0 0 1 \n",
      "y 17001 14877 12534 \n",
      "s 53841 45962 43915 \n",
      "D 491 227 334 \n",
      "k 4277 3707 5204 \n",
      "; 1354 2662 1143 \n",
      "Υ 0 0 1 \n",
      "à 10 0 0 \n",
      "b 13245 9611 10636 \n",
      "P 442 365 320 \n",
      "Ν 0 0 1 \n",
      "C 395 308 439 \n",
      "u 26311 21025 19519 \n",
      "f 22354 18351 16272 \n",
      "S 729 578 841 \n",
      "Y 282 234 111 \n",
      "I 4846 4917 3480 \n",
      "α 0 0 2 \n",
      "T 2217 1230 1583 \n",
      "W 739 681 732 \n",
      "g 16088 12601 14951 \n",
      "v 9624 7948 6529 \n",
      "F 383 232 269 \n",
      "K 86 35 176 \n",
      "U 166 46 94 \n",
      ". 8406 5761 5908 \n",
      "G 313 246 318 \n",
      "ü 1 0 5 \n",
      "e 114885 97515 88259 \n",
      "ï 0 0 7 \n",
      "j 683 682 424 \n",
      "L 458 307 249 \n",
      "Å 0 0 1 \n",
      "c 24127 17911 18338 \n",
      "a 68525 55274 56815 \n",
      "\" 2987 1469 513 \n",
      "é 47 0 15 \n",
      "ô 8 0 0 \n",
      "h 51580 43738 42770 \n",
      "x 1951 1267 1061 \n",
      "ä 1 0 6 \n",
      "? 510 419 169 \n",
      "è 15 0 0 \n",
      "V 156 57 67 \n",
      "Ο 0 0 3 \n",
      "A 1258 943 1167 \n",
      "Q 21 7 10 \n",
      "X 17 4 5 \n",
      "p 17422 12361 10965 \n",
      "î 1 0 0 \n",
      "ç 1 0 0 \n",
      "ë 0 0 12 \n",
      "δ 0 0 2 \n",
      "B 835 395 533 \n",
      "o 67145 53386 50996 \n",
      "w 17507 16062 15554 \n",
      "Æ 1 0 4 \n",
      ", 17594 12045 8581 \n",
      "z 634 400 529 \n",
      "d 36862 35315 33366 \n",
      "r 51221 44042 40590 \n",
      "R 258 385 237 \n",
      "Z 23 2 51 \n",
      "ê 28 0 2 \n",
      "O 414 282 503 \n",
      "ö 16 0 3 \n",
      ": 176 339 47 \n"
     ]
    }
   ],
   "source": [
    "counter = {name : defaultdict(int) for name in set(df.author)}\n",
    "for (text, author) in zip(df.text, df.author):\n",
    "    text = text.replace(' ', '')\n",
    "    for c in text:\n",
    "        counter[author][c] += 1\n",
    "\n",
    "chars = set()\n",
    "for v in counter.values():\n",
    "    chars |= v.keys()\n",
    "    \n",
    "names = [author for author in counter.keys()]\n",
    "\n",
    "print('c ', end='')\n",
    "for n in names:\n",
    "    print(n, end='   ')\n",
    "print()\n",
    "for c in chars:    \n",
    "    print(c, end=' ')\n",
    "    for n in names:\n",
    "        print(counter[n][c], end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of character distribution\n",
    "1. HPL and EAP used non ascii characters like a ä.\n",
    "2. The number of punctuations seems to be good feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "My preproceeings are\n",
    "\n",
    "1. Separate punctuation from words\n",
    "2. Remove lower frequency words ( <= 2)\n",
    "3. Cut a longer document which contains 256 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "        ngrams = []\n",
    "        for n in range(2, n_gram_max+1):\n",
    "            for w_index in range(len(q)-n+1):\n",
    "                ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "        return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocess(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "\n",
    "docs = create_docs(df)\n",
    "tokenizer = Tokenizer(lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model: FastText by Keras\n",
    "\n",
    "FastText is very fast and strong baseline algorithm for text classification based on Continuous Bag-of-Words model a.k.a Word2vec.\n",
    "\n",
    "FastText contains only three layers:\n",
    "\n",
    "1. Embeddings layer: Input words (and word n-grams) are all words in a sentence/document\n",
    "2. Mean/AveragePooling Layer: Taking average vector of Embedding vectors\n",
    "3. Softmax layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My FastText parameters are:\n",
    "\n",
    "* The dimension of word vector is 20\n",
    "* Optimizer is Adam\n",
    "* Inputs are words and word bi-grams\n",
    "    * you can change this parameter by passing the max n-gram size to argument of create_docs function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = np.max(docs) + 1\n",
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(embedding_dims=20, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 45s - loss: 1.0675 - acc: 0.4076 - val_loss: 1.0300 - val_acc: 0.4545\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 45s - loss: 0.9335 - acc: 0.6116 - val_loss: 0.8598 - val_acc: 0.7148\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 45s - loss: 0.7258 - acc: 0.7869 - val_loss: 0.7078 - val_acc: 0.7577\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 44s - loss: 0.5665 - acc: 0.8473 - val_loss: 0.6102 - val_acc: 0.7875\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 44s - loss: 0.4530 - acc: 0.8798 - val_loss: 0.5407 - val_acc: 0.8044\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 44s - loss: 0.3669 - acc: 0.9065 - val_loss: 0.4882 - val_acc: 0.8197\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 46s - loss: 0.2994 - acc: 0.9280 - val_loss: 0.4510 - val_acc: 0.8297\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 45s - loss: 0.2455 - acc: 0.9445 - val_loss: 0.4193 - val_acc: 0.8447\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 45s - loss: 0.2015 - acc: 0.9579 - val_loss: 0.3983 - val_acc: 0.8460\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 45s - loss: 0.1659 - acc: 0.9665 - val_loss: 0.3854 - val_acc: 0.8486\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 43s - loss: 0.1361 - acc: 0.9743 - val_loss: 0.3659 - val_acc: 0.8593\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 46s - loss: 0.1122 - acc: 0.9801 - val_loss: 0.3590 - val_acc: 0.8634\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 44s - loss: 0.0924 - acc: 0.9848 - val_loss: 0.3479 - val_acc: 0.8687\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 44s - loss: 0.0766 - acc: 0.9875 - val_loss: 0.3430 - val_acc: 0.8685\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 47s - loss: 0.0632 - acc: 0.9905 - val_loss: 0.3410 - val_acc: 0.8682\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 43s - loss: 0.0525 - acc: 0.9921 - val_loss: 0.3413 - val_acc: 0.8698\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 43s - loss: 0.0436 - acc: 0.9937 - val_loss: 0.3459 - val_acc: 0.8667\n",
      "Epoch 18/25\n",
      "15663/15663 [==============================] - 43s - loss: 0.0363 - acc: 0.9951 - val_loss: 0.3422 - val_acc: 0.8675\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Change Preprocessings\n",
    "\n",
    "## 2.1.1 Do lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = create_docs(df)\n",
    "tokenizer = Tokenizer(lower=True, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "maxlen = 256\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "\n",
    "input_dim = np.max(docs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/16\n",
      "15663/15663 [==============================] - 43s - loss: 1.0696 - acc: 0.4107 - val_loss: 1.0327 - val_acc: 0.4770\n",
      "Epoch 2/16\n",
      "15663/15663 [==============================] - 42s - loss: 0.9472 - acc: 0.5842 - val_loss: 0.8711 - val_acc: 0.6665\n",
      "Epoch 3/16\n",
      "15663/15663 [==============================] - 43s - loss: 0.7529 - acc: 0.7696 - val_loss: 0.7188 - val_acc: 0.7865\n",
      "Epoch 4/16\n",
      "15663/15663 [==============================] - 42s - loss: 0.5936 - acc: 0.8343 - val_loss: 0.6116 - val_acc: 0.7960\n",
      "Epoch 5/16\n",
      "15663/15663 [==============================] - 43s - loss: 0.4781 - acc: 0.8696 - val_loss: 0.5364 - val_acc: 0.8210\n",
      "Epoch 6/16\n",
      "15663/15663 [==============================] - 56s - loss: 0.3911 - acc: 0.8993 - val_loss: 0.4856 - val_acc: 0.8220\n",
      "Epoch 7/16\n",
      "15663/15663 [==============================] - 57s - loss: 0.3216 - acc: 0.9198 - val_loss: 0.4421 - val_acc: 0.8511\n",
      "Epoch 8/16\n",
      "15663/15663 [==============================] - 57s - loss: 0.2656 - acc: 0.9383 - val_loss: 0.4069 - val_acc: 0.8555\n",
      "Epoch 9/16\n",
      "15663/15663 [==============================] - 55s - loss: 0.2198 - acc: 0.9529 - val_loss: 0.3820 - val_acc: 0.8626\n",
      "Epoch 10/16\n",
      "15663/15663 [==============================] - 46s - loss: 0.1820 - acc: 0.9628 - val_loss: 0.3644 - val_acc: 0.8677\n",
      "Epoch 11/16\n",
      "15663/15663 [==============================] - 43s - loss: 0.1515 - acc: 0.9703 - val_loss: 0.3499 - val_acc: 0.8693\n",
      "Epoch 12/16\n",
      "15663/15663 [==============================] - 53s - loss: 0.1258 - acc: 0.9762 - val_loss: 0.3407 - val_acc: 0.8667\n",
      "Epoch 13/16\n",
      "15663/15663 [==============================] - 61s - loss: 0.1047 - acc: 0.9800 - val_loss: 0.3369 - val_acc: 0.8664\n",
      "Epoch 14/16\n",
      "15663/15663 [==============================] - 53s - loss: 0.0877 - acc: 0.9851 - val_loss: 0.3259 - val_acc: 0.8698\n",
      "Epoch 15/16\n",
      "15663/15663 [==============================] - 36s - loss: 0.0728 - acc: 0.9881 - val_loss: 0.3218 - val_acc: 0.8710\n",
      "Epoch 16/16\n",
      "15663/15663 [==============================] - 40s - loss: 0.0610 - acc: 0.9906 - val_loss: 0.3129 - val_acc: 0.8777\n"
     ]
    }
   ],
   "source": [
    "epochs = 16\n",
    "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n",
    "\n",
    "model = create_model()\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 batch_size=16,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=epochs,\n",
    "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7392/8392 [=========================>....] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('../Data/test.csv')\n",
    "docs = create_docs(test_df)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "y = model.predict_proba(docs)\n",
    "\n",
    "result = pd.read_csv('../Data/sample_submission.csv')\n",
    "for a, i in a2c.items():\n",
    "    result[a] = y[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.to_csv('fastText_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
